# -*- coding: utf-8 -*-
"""Potato Leaf Disease Classification.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1s23DsANOgvIJwHtG3FqT5yB11L3_ETYg
"""

#import necessary libraries
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import cv2
import PIL
import tensorflow as tf
from tensorflow.keras.layers import Activation, Dense, Flatten, Input, Conv2D, MaxPooling2D, Dropout, BatchNormalization
from tensorflow.keras.models import Model
from tensorflow.keras.models import Sequential
from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.keras.applications.resnet import ResNet152, preprocess_input, decode_predictions, ResNet50
from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve
from tensorflow.keras.utils import plot_model
from tensorflow.keras.preprocessing.image import img_to_array, load_img, array_to_img
import seaborn as sns

# download data
! kaggle datasets download nirmalsankalana/potato-leaf-healthy-and-late-blight
! unzip /content/potato-leaf-healthy-and-late-blight.zip

#create directory to store all data
! mkdir allData

#move the two existing directories into the allData directory
! mv 'Late Blight' allData
! mv 'Healthy' allData

#store the directory's path in data_dir
import pathlib
path = "/content/allData"
data_dir = pathlib.Path(path).with_suffix('')

image_count = len(list(data_dir.glob('*/*.jpg')))
print(image_count)

healthy = list(data_dir.glob("Healthy/*"))
diseased = list(data_dir.glob("Late Blight/*"))
print("Number of healthy cells", len(healthy))
print("Number of diseased cells", len(diseased))

PIL.Image.open(healthy[10])

PIL.Image.open(diseased[10])

#put all images (from both classes) into a single list to make splitting easier
dataset = list(data_dir.glob('*/*.jpg'))
from sklearn.model_selection import train_test_split
train, test = train_test_split(dataset, test_size=0.2)

#make directories to store the images from the train test split
! mkdir trainingData
! mkdir testingData

#put all images in the (train & test) lists into their respective directories
for i in train:
  !cp "{i}" trainingData

for i in test:
  !cp "{i}" testingData

#storing the path for each directory

train_path = "/content/trainingData"
train_data_dir = pathlib.Path(train_path).with_suffix('')

test_path = "/content/testingData"
test_data_dir = pathlib.Path(test_path).with_suffix('')

print("Number of images in training datset:", len(list(train_data_dir.glob('*.jpg'))))
print("Number of images in test datset:", len(list(test_data_dir.glob('*.jpg'))))

#making lists to seperate the classes in each of the training and test directories

healthy_training = list(train_data_dir.glob('Healthy*.jpg'))
diseased_training = list(train_data_dir.glob('Late_Blight*.jpg'))

healthy_testing = list(test_data_dir.glob('Healthy*.jpg'))
diseased_testing = list(test_data_dir.glob('Late_Blight*.jpg'))

#making directories for each class in training and testing directories
! mkdir trainingData/healthy_training
! mkdir trainingData/diseased_training

! mkdir testingData/healthy_testing
! mkdir testingData/diseased_testing

#moving the images from the previously made lists to the respective directories

for i in healthy_training:
  !mv "{i}" trainingData/healthy_training

for i in diseased_training:
  !mv "{i}" trainingData/diseased_training


for i in healthy_testing:
  !mv "{i}" testingData/healthy_testing

for i in diseased_testing:
  !mv "{i}" testingData/diseased_testing

batch_size = 32
img_height = 256
img_width = 256

train_gen = tf.keras.preprocessing.image.ImageDataGenerator(
    rotation_range=40,
    shear_range=0.2,
    zoom_range=0.2,
    horizontal_flip=True,
    validation_split = 0.3,
    rescale = 1/255
)

test_gen = tf.keras.preprocessing.image.ImageDataGenerator(
    rescale = 1/255
)

training_data = train_gen.flow_from_directory(
    train_data_dir,
    subset = "training",
    class_mode="binary",
    target_size = (img_width, img_height),
    batch_size = batch_size,
    shuffle = True #because we want to train it with randomly shuffled data
)

validation_data = train_gen.flow_from_directory(
    train_data_dir,
    subset = "validation",
    class_mode="binary",
    target_size = (img_width, img_height),
    batch_size = batch_size,
    shuffle = False
)

test_data = test_gen.flow_from_directory(
    test_data_dir,
    class_mode="binary",
    target_size = (img_width, img_height),
    batch_size = batch_size,
    shuffle = False
)

image_shape = (256,256,3)

model = Sequential()

model.add(Conv2D(filters=64, kernel_size=(3,3),input_shape=image_shape, activation='relu',))
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(BatchNormalization())

model.add(Conv2D(filters=128, kernel_size=(3,3), activation='relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(BatchNormalization())

model.add(Conv2D(filters=128, kernel_size=(3,3), activation='relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(BatchNormalization())

model.add(Conv2D(filters=128, kernel_size=(3,3), activation='relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(BatchNormalization())

model.add(Conv2D(filters=64, kernel_size=(3,3), activation='relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))

model.add(Flatten())

model.add(Dense(64, activation='relu'))
model.add(BatchNormalization())

model.add(Dense(1, activation='sigmoid'))

model.compile(loss='binary_crossentropy',
              optimizer='adam',
              metrics=['accuracy'])

plot_model(model, to_file="model.png", show_shapes = True, show_layer_names= True)

model.summary()

early_stop = EarlyStopping(monitor='val_loss',patience=10)

results = model.fit(training_data,epochs=15,
                    validation_data=validation_data,
                    callbacks=[early_stop])

summary = pd.DataFrame(model.history.history)
summary.head(15)

plt.figure(figsize=(10,6))
plt.plot(summary.loss, label="loss")
plt.plot(summary.val_loss, label="val_loss")
plt.legend(loc="upper right")
plt.ylabel("Loss")
plt.xlabel("Epoch")
plt.show()

plt.figure(figsize=(10,6))
plt.plot(summary.accuracy, label="accuracy")
plt.plot(summary.val_accuracy, label="val_accuracy")
plt.legend(loc="upper left")
plt.ylabel("Accuracy")
plt.xlabel("Epoch")
plt.show()

model.evaluate(training_data)

model.evaluate(test_data)

pred_probabilities = model.predict(test_data)

predictions = pred_probabilities > 0.5

print(classification_report(test_data.classes,predictions))